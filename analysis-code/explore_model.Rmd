---
title: "Evictions v. Homelessness Analysis"
subtitle: "Sarah Jiang"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE)
```

------------------------------------------------------------------------

## Panel Dataset

| variable     | label                                                                        |
|------------------|------------------------------------|
| state_fips   | state FIPS                                                                   |
| state_name   | state name                                                                   |
| state_abbrev | state abbreviation                                                           |
| year         | year of observed homeless rate                                               |
| fileyear     | = year - 1; year of observed filing rate                                     |
| filerate     | Evictions filing rate per 1,000 renting households.                          |
| homelesscap  | Rate of homelessness per 1,000 residents.                                    |
| chroniccap   | Chronic homelessness rate per 1,000 residents.                               |
| red          | = 1 if proportion of Republican wins \> 60% for Senate elections 1976 - 2020 |
| blue         | = 1 if proportion of Democratic wins \> 60% for Senate elections 1976 - 2020 |
| densityrank  | 2010 Census historical population density ranking                            |
| dense        | = 1 if 2010 Census population density ranking is 26 or lower                 |

```{r load-libs, collapse=TRUE, hide = TRUE}
library(haven)
library(MASS)
library(ggplot2)
data = read_dta("homelesspanel.dta")
```

## Summarize, explore & visualize data

The dataset comprises of 452 observations of eviction filings per 1,000
renting households and number of homeless people per 1,000 residents per
state per year from 2008 to 2019. Not all states and years are
represented. Variables red, blue, densityrank and dense are for the
purpose of profiling states and do not vary within each state.

The variable fileyear = year - 1 to stagger filerate and homelesscap as
suggested by my professor, since the effects of evictions may not have
an immediately observable effect on the state's population of homeless.

```{r}
head(data)
sumstats = summary(data[c('filerate', 'homelesscap', 'chroniccap', 'red', 'blue')])
print(sumstats)
```

Let's take an initial look at the distribution of our main variables of interest, homelesscap and filerate.
```{r}
par(mfrow = c(1, 2))

hist(data$homelesscap, breaks = 40, main = "Homelessness by state by year (homelesscap)", xlab = 'homeless persons per 1,000 residents', col = 'seashell', cex.main = 0.7, cex.lab = 0.7)

hist(data$filerate, breaks = 40, main = 'Rate of eviction filings by state by year (filerate)', xlab = 'eviction filings per 1,000 renting households', col = 'lavender', cex.main = 0.7, cex.lab = 0.7)
```
```{r}
par(mfrow = c(1, 2))

boxplot(data$homelesscap, main = "Homelessness by state by year (homelesscap)", ylab = 'homeless persons per 1,000 residents', cex.main = 0.7, cex.lab = 0.7, col = 'seashell')

boxplot(data$filerate, main = 'Eviction filings by state by year (filerate)', ylab = 'eviction filings per 1,000 renting households', cex.main = 0.7, cex.lab = 0.7, col = 'lavender')
```

The overall distributions of homelesscap and filerate are both very
right-skewed and likely multimodal due to states with unique
circumstances (the high outlier on filing rates is Maryland). 

For now I'm just going to have to do my best with what I know. \*\*
correlation..??? covariance???

## Exploring homelesscap v. filerate

Is there any relationship between eviction filing rates and
homelessness?

```{r}
base = ggplot(data = data, mapping = aes(x = filerate, y = homelesscap))

base + geom_point(size = 0.8) + 
  geom_smooth(method = 'lm', color = 'black', linewidth = .7) +
  geom_smooth(color = 'green', linewidth = .7) +
  labs(title = 'Scatterplot of rates of evictions filings & rates of homelessness by state by year') +
  ylab('# homeless persons per 1,000 residents') +
  xlab('# eviction filings per 1,000 renting households')

```

A linear model (illustrated by the straight black line) would clearly not be a good fit for the distribution of our data. It would also violate assumptions about heteroskedasticity and the Conditional Mean Assumption, $E(u|x) = 0$. As filerate increases, the variance of the errors decreases. $E(u|x)$ fluctuates as $x$, or filerate, changes, because as filerate increases, we know that $E(u)$ will first become negative around $150 < x < 300$ before becoming positive when $x > 350$. 

To account for these assumption violations, I will incorporate robust standard errors and control variables in my models.

$~$

### Initial simple linear regression

$$\hat{homelesscap}_i = \hat\beta_0 + \hat\beta_1{filerate}_i + u_i$$

```{r}
l_model = lm(homelesscap ~ filerate, data = data)
summary(l_model)

# Plot model diagnostics
par(mfrow=c(1, 2))
plot(l_model, which = 1:2)
mtext('Simple linear regression model diagnostics', side = 3, line = -2, outer = TRUE)
```

From the scatterplot of fitted values and residuals, we can see a clear downward trend in the residual values, showing that the variables $u$ and $\hat{y}$ are not independent. The QQ plot also shows that our error terms are not Normally distributed. These plots confirm that this simple linear specification does not fit our data well.

Let's add some controls to our model.

$~$

### Multivariate linear regression

$$\hat{homelesscap}_i = \hat\beta_0 + \hat\beta_1{filerate}_i + \hat\beta_2{red}_i + \hat\beta_3{blue}_i + \hat\beta_4{densityrank}_i + u_i$$
****** is it ok to have both red and blue in the model though
```{r}
lc_model = lm(homelesscap ~ filerate + red + blue + densityrank, data = data)
summary(lc_model)

par(mfrow = c(1, 2))
plot(lc_model, which = 1:2)
mtext('Multivariate linear regression model diagnostics', side = 3, line = -2, outer = TRUE)
```

Although the distribution of residuals is still far from what's desirable, the fit of this model is much improved compared to the previous one judging from the $R^2$. Several of the coefficients in this model are significantly different from 0. 

Calculating robust standard errors; however if the model is wrong they won't be useful
```{r}
library(lmtest)
library(sandwich)

coeftest(lc_model, vcov = vcovHC, type = "HC1")

waldtest(lc_model, terms = "filerate", vcov = vcovHC)

confint(lc_model, "filerate", vcov = vcovHC)

```

not sure if my specification is ok. or if we even care about robust standard errors since we almost certainly don't have the right model. and we're probbly using the wrong type of robust standard error (HC1). but the hypothesis test and confidence intervals we got suggest that the effect that filerate has one homelesscap is slightly negative and significantly different from 0.

$~$

### Quadratic regression

```{r}
l2_model = lm(homelesscap ~ filerate + filerate^2 + blue + densityrank, data = data)
summary(l2_model)
```

```{r, echo - FALSE}
par(mfrow = c(1,2))
plot(l2_model, which = 1:2)
mtext('quadratic model diags', side = 3, line = -2, outer = TRUE)
```

interpretation fo coefficients more complicated.

$~$

### Log-log regression

$$\hat{ln(homelesscap)}_i = \hat\beta_0 + \hat\beta_1{ln(filerate)}_i + \hat\beta_2{red}_i + \hat\beta_3{blue}_i + \hat\beta_4{densityrank}_i + u_i$$

Try transforming the data using a log-log model. Unfortunately, we
have some zeros in filerate so we'll need to drop the corresponding observations. also wont it screw up the interpretation to have logged y and x1 but not the controls..?

```{r}
data_nozero = subset(data, (filerate>0) & (homelesscap>0))

ln_model = lm(log(homelesscap) ~ log(filerate) + red + blue + densityrank, data = data_nozero)
summary(ln_model)
```
```{r, echo = FALSE}
par(mfrow = c(1,2))
plot(ln_model, which = 1:2)
mtext('ln(homelesscap) = A + B(ln(filerate)) + controls + u', side = 3, line = -2, outer = TRUE)
```


From looking at the plots the fit seems to be much improved;
distribution of residuals is much less crazy & heteroskedasticity seems
to be reduced significantly, although it is still present. The coefficients indicate that according to this model, for every 1% change in x y changes by -.02% (???) on average, holding state politics and population density constant. however, the result is not statistically significant and our $R^2$ has decreased from our last model despite the residuals on this model behaving more.

Coefficients here suggest no significant relationship between filerate and homelesscap.

$~$

## State-fixed effects

```{r}
dlabs = c('1' = 'High Density', '0' = 'Low Density')
blabs = c('1' = 'Blue state', '0' = 'Red or Purple state')

base + geom_point(size = 0.8) +
  geom_smooth(method = 'lm', color = 'black', linewidth = .7) +
  geom_smooth(color = 'green', linewidth = .7) +
  facet_grid(rows = vars(dense), cols = vars(blue), 
             labeller = labeller(blue = blabs, dense = dlabs))
```

The relationship between filing rate and homelessness seems to vary
wildly based on state characteristics. For this reason, a fixed effects
model could work better for our data.

$~$

### Least squares dummy variable model

$$\hat{homelesscap}_{it} = \hat\beta_0(?) + \hat\beta_1{filerate}_{it} + \hat\gamma_{AK}{DAK}_i + ... + \hat\gamma_{WY}{DWY}_i + u_{it}$$

```{r}
lsdv_model = lm(homelesscap ~ filerate + factor(state_abbrev)-1, data = data)
#summary(lsdv_model)
```

not sure whether to omit intercept or not? bc i dont want to imply that
homeless = 0 when filerate = 0 but also would it cause
multicollinearity? filerate effect still insignificant but state
intercepts significant. would dropping a state dummy instead achieve the same effect?

need to truncate output. i can keep a table of all the coefficients somewhere else

```{r, echo = FALSE}
par(mfrow=c(1, 2))

plot(lsdv_model, which = 1:2)
```

$~$

### Fixed-effects model (within)



```{r}
library(plm)
fe_model = plm(log(homelesscap) ~ log(filerate), data = data_nozero, index = c('state_abbrev', 'year'), model = 'within')
summary(fe_model)

```

this version of fixed effects seems to not work very well at all lmao.
very high p. except when logged

-   introduce state fixed effects \*\* initial fe reg: explain overall,
    between and within variation \*\* do a hausman test seems easy
    enough \*\* focus on within variation?? or time demean? \*\* maybe
    also a first differences test?
-   non linearity (how to combine nonlinearity with fixed effects?) \*\*
    EXPORT SOME TABLES?
-   find some tests to do to figure out best model ig

## Discussion & conclusion

We could look more into our sample and data sources. we already know that the PIT counts
are flawed measures, plus we could further investigate our panel dataset
to see why it is unbalanced and doesnt have all states represented.

outliers handled w fe? or remove?

** dont include all diagnostic graphs in this notebook. put them in a different script for those interested
